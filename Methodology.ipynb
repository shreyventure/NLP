{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology:\n",
    "\n",
    "\"Though LLMs have been pre-trained on massive of text encompassing mathematical concepts and problems, LLMs may still exhibit deficiencies in solving math-related tasks\"\n",
    "\n",
    "\"Our goal is to enhance the models’ ability to generalize the learned rules across all inputs related to the knowledge.\"\n",
    "\n",
    "## Traditional learn-from-example models:\n",
    "\n",
    "`θ∗ = argmin(θ) E(x,y)∼D[L(f(x; θ), y)]`, \n",
    "where `θ∗` is the optimal model parameter, `f(x; θ)` denotes the output of LLM on the input query `x` and `L` is the loss function.\n",
    "#### Disadvantages:\n",
    "This learning paradigm may face problems when the task rule\n",
    "- is complex and challenging to capture, \n",
    "especially when \n",
    "- the quantity of training samples is limited.\n",
    "\n",
    "## New learn-from-rules model:\n",
    "\n",
    "T -> Task <br/>\n",
    "RT -> rules for task T (RT can be a task instruction)\n",
    "\n",
    "`θ∗ = argmin(θ) LR(f(θ), q(RT))`\n",
    "where `q(RT)` is the optimal model that can reflect the rule RT , LR is the loss function measuring the difference between two models. .<br/>\n",
    "Aquiring `q(RT)` is the process of translating the knowledge embedded within textual rules into learning signals that LLMs can effectively decode and apply.\n",
    "\n",
    "### Rule Distilation\n",
    "In practice, a model `q(RT)` still lacks robustness and effectivity. Recent research has revealed that LLMs are better at understanding and exeuting new tasks when provided with detailed task description or instructions as promts, thanks to their advanced in-context learning capabilities. So for the task-rule `RT`, for task `T`, `f(RT ; θ)` may be a good alternative for the optimal model `q(RT)`.\n",
    "The equation can be reforulated as,\n",
    "`θ∗ = argmin(θˆ)E(x,y)∼D[L(f(x; θˆ), f(RT , x; θ))].`\n",
    "\n",
    "1. Distilling from In-Context Textual Rules with Output Distributions (minimizes KL divergence)<br/>\n",
    "\n",
    "Next, we perform knowledge distillation mechanism to minimize Kullback-Leibler divergence between the target model θˆ and base model θ. <br/>\n",
    "We do not aim to distill the knowledge that is already stored in the base model’s parameters, but we attempt to explicitly encode the knowledge in the textual rules into the target model by distilling the mechanisms behind the actions the base model take after it understands the textual rules with in-context learning ability. Therefore, we call it the rule distillation method.\n",
    "\n",
    "2. Distilling from In-Context Textual Rules with Hidden Signals(fully utilize the information of the base model produced on responding on the RT) <br/>\n",
    "\n",
    "Next, we align  the hidden states of each layer between two models given different inputs. In this way, we can make the target model learn the rules more thoroughly by learning from the full thinking procedures of the base model responded to the task rule RT .\n",
    "\n",
    "3. Enhancing LLM’s Understanding of Textual Rules<br/>\n",
    "\n",
    "The in-context ability of LLM depends on several conditions, such as the scale of LLM and the quality of the instruction text Rt, it usually happens that the LLM cannot well understand the given textual rule, and therefore, there are some `y′ = f(Rt, x|θ)` that are not correctly generated by the base model.\n",
    "\n",
    "We first correct the wrong `f(Rt, x|θ)` manually, then prepend the inputs with Rt and use the correct responses to perform the example-based learning on the base model for some optimization steps.\n",
    "\n",
    "Experiemnts:\n",
    "\n",
    "1. Arithmetic\n",
    "2. The Backdoor Task\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
